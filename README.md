<h1 align="center">
    Arithmetics with âœ¨ Machine Learning âœ¨
</h1>

<p align="center">
    A <i>silly</i> use-case of machine learning model. Watch <b>ML</b> âš¡ in <i>action!</i> 
</p>


## ğŸ¤” What is it?
Implementation of simple arithmetic operationsâ€”multiplication, addition, and subtractionâ€”on an array of natural numbers with a single-neuron deep learning network (basically _linear regression!_). 

`main` branch consists of manual implementation of the model with **numpy**.

`pytorch` branch implements the above model using the **pytorch** library.

## ğŸš€ [Open with Streamlit](https://share.streamlit.io/yashdeep01/arithmetic-ml-model/main)
Visualise the gradient descent and linear regression happening right in front of you on your browser. Tune the hyperparamters as you watch your model **learn** *live!* 

[![Demo](./assets/images/streamlit-streamlit_app-2021-07-20-15-07-10.gif)](https://share.streamlit.io/yashdeep01/arithmetic-ml-model/main)

## ğŸ’» Running with CLI
### Create conda environment ğŸ
> Install the anaconda package from [here](https://docs.anaconda.com/anaconda/install/) and run these commands on terminal:
```
conda init
conda create -n arithmetic-ml python=3.8
conda activate arithmetic-ml
```
### Clone this repo ğŸ”—
```
git clone https://github.com/yashdeep01/arithmetic-ml-model.git
cd arithmetic-ml-model/
pip install -r requirements.txt
```
### Run ğŸ› ï¸
```
python main.py
```
Run with custom hyperparameters:
```
python main.py --operand 2 --lr 0.018 --epochs 20
```
Get full list of command-line arguments:
```
python main.py -h
```

---

## â„¹ï¸ About the model
A single neuron deep learning neural network with identity activation function; in other words, *linear regression*. 

**Input** `1,2,3,...,n`  
**Target** `c,2c,3c,...nc`, *where c is a constant operand*

### âš™ï¸ Training 
Steps in each epoch:

1. **Forward propagation** â¡ï¸  
Computes the standard equation `w*X + b` in forward propagation. Identity activation function.

2. **Loss computation** âš ï¸  
Calculates Mean Squared Error (MSE) loss at the end of the forward propagation.

3. **Back propagation** â¬…ï¸  
Gradient of loss function with respect to weight `w` and bias `b` obtained: `dw` and `db` respectively.

4. **Gradient descent** ğŸ“‰  
Updates weight and bias by stepping against the gradient direction in the magnitude of learning rate `lr`.
    ```
    w' = w - lr*dw
    b' = b - lr*db
    ```
Since training dataset is small, batch size is the full training set.

### ğŸ Validation
Predicts targets for numbers in validation set using the final weight and bias, then reports validation loss.

## ğŸ“š Acknowledgement
Inspired from tutorial: https://github.com/python-engineer/pytorchTutorial
